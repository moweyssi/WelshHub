{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding PDF files using OpenAI text-embedding-3-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "%env OPENAI_API_KEY=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\n",
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF files from selected folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the folder containing PDFs\n",
    "path = r\"C:\\Users\\maxim.oweyssi\\Energy Saving Trust\\Lutz Lemmer - Sample_AI_Chatbot_Documents\"\n",
    "\n",
    "# Use glob to get all PDF files in the specified folder\n",
    "pdf_files = glob.glob(os.path.join(path, '*.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text and parse into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text_by_page = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text_by_page[page_num + 1] = page.get_text()\n",
    "    doc.close()\n",
    "    return text_by_page\n",
    "\n",
    "def identify_pages(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_text = {}\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "\n",
    "        # Use get_text(\"blocks\") to get text blocks\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "\n",
    "        # Calculate the threshold as 1/10th of the page length from the top and bottom\n",
    "        page_height = page.rect.height\n",
    "        threshold = page_height / 10\n",
    "\n",
    "        # Filter out blocks likely to be headers or footers based on their vertical position\n",
    "        filtered_blocks = [block for block in blocks if block[3] > threshold and block[3] < (page_height - threshold)]\n",
    "\n",
    "        # Extract text from filtered blocks\n",
    "        page_text = \" \".join([block[4] for block in filtered_blocks])\n",
    "\n",
    "        # Check if the page text is not empty before adding to the dictionary\n",
    "        if page_text.strip():\n",
    "            pages_text[f'Page {page_num + 1}'] = page_text\n",
    "\n",
    "    doc.close()\n",
    "    return pages_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim.oweyssi\\Energy Saving Trust\\Lutz Lemmer - Sample_AI_Chatbot_Documents\\EPCs-as-efficiency-targets-v91.pdf\n",
      "C:\\Users\\maxim.oweyssi\\Energy Saving Trust\\Lutz Lemmer - Sample_AI_Chatbot_Documents\\lc4r_draft_final_report.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim.oweyssi\\Energy Saving Trust\\Lutz Lemmer - Sample_AI_Chatbot_Documents\\Regen-Heat-Paper.pdf\n"
     ]
    }
   ],
   "source": [
    "all_pages = []\n",
    "all_page_numbers = []\n",
    "all_document_names = []\n",
    "for pdf_filepath in pdf_files:\n",
    "    print(pdf_filepath)\n",
    "    text_by_page = identify_pages(pdf_filepath)\n",
    "    # Extract paragraphs and tables\n",
    "    doc_pages = [text_by_page[page_key] for page_key in text_by_page]\n",
    "    all_pages.extend(doc_pages)\n",
    "    all_page_numbers.extend([page_num for page_num in text_by_page])\n",
    "    all_document_names.extend([os.path.basename(pdf_filepath)]*len(doc_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create embedding using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model,dimensions=512).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run embedding and save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "df = pd.DataFrame({'Document':all_document_names, 'Page':all_page_numbers,'Text':all_pages})\n",
    "df['ada_embedding'] = df['Text'].apply(lambda x: get_embedding(x))\n",
    "df.to_csv('embedding.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
