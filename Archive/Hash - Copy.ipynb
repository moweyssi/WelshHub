{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import PyPDF2\n",
    "import glob\n",
    "import fitz  # PyMuPDF\n",
    "import tabula\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "from fuzzywuzzy import process\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the folder containing PDFs\n",
    "path = r\"C:\\Users\\maxim.oweyssi\\Energy Saving Trust\\Lutz Lemmer - Sample_AI_Chatbot_Documents\"\n",
    "\n",
    "# Function to extract text from a PDF using PyPDF2\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "# Use glob to get all PDF files in the specified folder\n",
    "pdf_files = glob.glob(os.path.join(path, '*.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text_by_page = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text_by_page[page_num + 1] = page.get_text()\n",
    "    doc.close()\n",
    "    return text_by_page\n",
    "\n",
    "def identify_paragraphs_and_tables(pdf_path,context_window_size = 5):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    paragraphs_by_page = {}\n",
    "    tables_by_page = {}\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "\n",
    "        # Use get_text(\"blocks\") to get text blocks\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "\n",
    "        paragraphs = []\n",
    "        current_paragraph = \"\"\n",
    "\n",
    "        # Calculate the threshold as 1/10th of the page length from the top and bottom\n",
    "        page_height = page.rect.height\n",
    "        threshold = page_height / 10\n",
    "\n",
    "        # Filter out blocks likely to be headers or footers based on their vertical position\n",
    "        filtered_blocks = [block for block in blocks if block[3] > threshold and block[3] < (page_height - threshold)]\n",
    "\n",
    "        # Extract text from filtered blocks\n",
    "        page_text = \" \".join([block[4] for block in filtered_blocks])\n",
    "\n",
    "        # Use sent_tokenize to split the text into sentences\n",
    "        sentences = sent_tokenize(page_text)\n",
    "        \n",
    "        for i in range(len(sentences) - context_window_size + 1):\n",
    "            # Concatenate the sentences from the context window\n",
    "            current_paragraph = \" \".join(sentences[i:i + context_window_size])\n",
    "\n",
    "            # Append the paragraph to the list\n",
    "            paragraphs.append(current_paragraph.strip())\n",
    "\n",
    "        # Table extraction using tabula-py\n",
    "        try:\n",
    "            tables = tabula.read_pdf(pdf_path, pages=page_num + 1, multiple_tables=True,encoding='latin1')\n",
    "            # Convert tables to pandas dataframes for further processing\n",
    "            tables_dataframes = [table for table in tables]\n",
    "            tables_by_page[page_num + 1] = tables_dataframes\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting tables on page {page_num + 1}: {e}\")\n",
    "            tables_by_page[page_num + 1] = []\n",
    "\n",
    "        paragraphs_by_page[page_num + 1] = paragraphs\n",
    "\n",
    "    doc.close()\n",
    "    return paragraphs_by_page, tables_by_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "pdf_filepath = pdf_files[0]\n",
    "paragraphs_by_page, tables_by_page = identify_paragraphs_and_tables(pdf_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf_vectors(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, extra whitespaces, and convert to lowercase\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "def search_term_cosine_similarity(query, documents, vectorizer):\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    document_vectors = vectorizer.transform(documents)\n",
    "    similarities = cosine_similarity(query_vector, document_vectors).flatten()\n",
    "    return similarities\n",
    "\n",
    "all_paragraphs = []\n",
    "all_page_numbers = []\n",
    "all_document_names = []\n",
    "all_tables = []\n",
    "for pdf_filepath in pdf_files:\n",
    "    print(pdf_filepath)\n",
    "    paragraphs_by_page, tables_by_page = identify_paragraphs_and_tables(pdf_filepath)\n",
    "    \n",
    "    # Extract paragraphs and tables\n",
    "    doc_paragraphs = [paragraph for page_num, page_paragraphs in paragraphs_by_page.items() for paragraph in page_paragraphs]\n",
    "    all_paragraphs.extend(doc_paragraphs)\n",
    "    all_page_numbers.extend([page_num for page_num, page_paragraphs in paragraphs_by_page.items() for paragraph in page_paragraphs])\n",
    "    all_document_names.extend([os.path.basename(pdf_filepath)]*len(doc_paragraphs))\n",
    "    all_tables.extend([table.to_string(index=False) for page_num, page_tables in tables_by_page.items() for table in page_tables])\n",
    "\n",
    "\n",
    "# Convert paragraphs to TF-IDF vectors\n",
    "paragraphs_vectorizer, paragraphs_tfidf_matrix = compute_tfidf_vectors([preprocess_text(paragraph) for paragraph in all_paragraphs])\n",
    "\n",
    "# Convert tables to TF-IDF vectors\n",
    "tables_vectorizer, tables_tfidf_matrix = compute_tfidf_vectors([preprocess_text(table) for table in all_tables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The EPC is populated using results from a SAP calculation. This is a static \n",
      "building physics modelling method derived from the Building Research \n",
      "Establishment’s Domestic Energy Model (BREDEM). It is less flexible \n",
      "than the BREDEM model itself as certain parameters are either fixed or \n",
      "restricted to a specific range. A SAP calculation will model heat loss, \n",
      "internal gains, solar gains, energy balance, carbon emissions, heating, \n",
      "ventilation, internal lighting, cooling and renewable energy sources. Building Regulations Part L1A sets out how SAP should be used to \n",
      "determine whether a new building will meet current building \n",
      "regulations.\n",
      "---------------------------------------\n",
      "This is a static \n",
      "building physics modelling method derived from the Building Research \n",
      "Establishment’s Domestic Energy Model (BREDEM). It is less flexible \n",
      "than the BREDEM model itself as certain parameters are either fixed or \n",
      "restricted to a specific range. A SAP calculation will model heat loss, \n",
      "internal gains, solar gains, energy balance, carbon emissions, heating, \n",
      "ventilation, internal lighting, cooling and renewable energy sources. Building Regulations Part L1A sets out how SAP should be used to \n",
      "determine whether a new building will meet current building \n",
      "regulations. There are several criteria – however, the most fundamental \n",
      "part of Part L1A’s approach is the ‘notional building’ method.\n",
      "---------------------------------------\n",
      "It is less flexible \n",
      "than the BREDEM model itself as certain parameters are either fixed or \n",
      "restricted to a specific range. A SAP calculation will model heat loss, \n",
      "internal gains, solar gains, energy balance, carbon emissions, heating, \n",
      "ventilation, internal lighting, cooling and renewable energy sources. Building Regulations Part L1A sets out how SAP should be used to \n",
      "determine whether a new building will meet current building \n",
      "regulations. There are several criteria – however, the most fundamental \n",
      "part of Part L1A’s approach is the ‘notional building’ method. This \n",
      "method uses a set of pre-defined performance characteristics1 which are \n",
      "applied to the shape/size and layout of a new building to determine a \n",
      "target emissions level.\n",
      "---------------------------------------\n",
      "A SAP calculation will model heat loss, \n",
      "internal gains, solar gains, energy balance, carbon emissions, heating, \n",
      "ventilation, internal lighting, cooling and renewable energy sources. Building Regulations Part L1A sets out how SAP should be used to \n",
      "determine whether a new building will meet current building \n",
      "regulations. There are several criteria – however, the most fundamental \n",
      "part of Part L1A’s approach is the ‘notional building’ method. This \n",
      "method uses a set of pre-defined performance characteristics1 which are \n",
      "applied to the shape/size and layout of a new building to determine a \n",
      "target emissions level. These performance characteristics have been \n",
      "chosen to reduce emissions levels by a certain percentage from a 1990 baseline and thus have been \n",
      "gradually tightened over the years.\n",
      "---------------------------------------\n",
      "Building Regulations Part L1A sets out how SAP should be used to \n",
      "determine whether a new building will meet current building \n",
      "regulations. There are several criteria – however, the most fundamental \n",
      "part of Part L1A’s approach is the ‘notional building’ method. This \n",
      "method uses a set of pre-defined performance characteristics1 which are \n",
      "applied to the shape/size and layout of a new building to determine a \n",
      "target emissions level. These performance characteristics have been \n",
      "chosen to reduce emissions levels by a certain percentage from a 1990 baseline and thus have been \n",
      "gradually tightened over the years. The SAP modelling method is used for both the notional and  \n",
      "proposed building.\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pagenum = 4\n",
    "for i in range(len(paragraphs_by_page[pagenum])):\n",
    "    print(paragraphs_by_page[pagenum][i])\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_document_names.pkl']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dump results into files\n",
    "joblib.dump(paragraphs_vectorizer, 'vectorizer.pkl', compress=True)\n",
    "joblib.dump(all_paragraphs, 'all_paragraphs.pkl', compress=True)\n",
    "joblib.dump(all_page_numbers, 'all_page_numbers.pkl', compress=True)\n",
    "joblib.dump(all_document_names, 'all_document_names.pkl', compress=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
